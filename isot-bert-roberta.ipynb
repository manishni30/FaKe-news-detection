{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-06T15:05:54.995882Z","iopub.status.busy":"2023-05-06T15:05:54.995076Z","iopub.status.idle":"2023-05-06T15:05:55.017029Z","shell.execute_reply":"2023-05-06T15:05:55.015898Z","shell.execute_reply.started":"2023-05-06T15:05:54.995845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/fake-and-real-news-dataset/True.csv\n","/kaggle/input/fake-and-real-news-dataset/Fake.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:06:25.579633Z","iopub.status.busy":"2023-05-06T15:06:25.579047Z","iopub.status.idle":"2023-05-06T15:06:32.735857Z","shell.execute_reply":"2023-05-06T15:06:32.734955Z","shell.execute_reply.started":"2023-05-06T15:06:25.579599Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import nltk\n","import re\n","import string\n","import seaborn as sns\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud,STOPWORDS\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:06:37.287869Z","iopub.status.busy":"2023-05-06T15:06:37.286996Z","iopub.status.idle":"2023-05-06T15:06:40.134057Z","shell.execute_reply":"2023-05-06T15:06:40.132609Z","shell.execute_reply.started":"2023-05-06T15:06:37.287831Z"},"trusted":true},"outputs":[],"source":["# load the dataset\n","df_true = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\n","df_fake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:06:43.269082Z","iopub.status.busy":"2023-05-06T15:06:43.268745Z","iopub.status.idle":"2023-05-06T15:06:43.309031Z","shell.execute_reply":"2023-05-06T15:06:43.308154Z","shell.execute_reply.started":"2023-05-06T15:06:43.269053Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/1587008257.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df = df_true.append(df_fake)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>text</th>\n","      <th>subject</th>\n","      <th>date</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Black U.S. lawmakers group endorses Clinton Wh...</td>\n","      <td>WASHINGTON (Reuters) - Democratic presidential...</td>\n","      <td>politicsNews</td>\n","      <td>February 11, 2016</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GEORGETOWN University Will Track Down, Recruit...</td>\n","      <td>Georgetown University has a 16.4% acceptance r...</td>\n","      <td>left-news</td>\n","      <td>Sep 1, 2016</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ex-CIA chief Brennan to testify before House I...</td>\n","      <td>WASHINGTON (Reuters) - Former Central Intellig...</td>\n","      <td>politicsNews</td>\n","      <td>May 18, 2017</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SHOCKING NUMBER OF MICHIGAN VOTERS Want MUSLIM...</td>\n","      <td>Thus the big win for Donald Trump in blue stat...</td>\n","      <td>politics</td>\n","      <td>Mar 8, 2016</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Douchebag of the Day: Louie ‘Self Loathing Je...</td>\n","      <td>Nothing pisses me off more than a Republican p...</td>\n","      <td>News</td>\n","      <td>March 12, 2016</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  \\\n","0  Black U.S. lawmakers group endorses Clinton Wh...   \n","1  GEORGETOWN University Will Track Down, Recruit...   \n","2  Ex-CIA chief Brennan to testify before House I...   \n","3  SHOCKING NUMBER OF MICHIGAN VOTERS Want MUSLIM...   \n","4   Douchebag of the Day: Louie ‘Self Loathing Je...   \n","\n","                                                text       subject  \\\n","0  WASHINGTON (Reuters) - Democratic presidential...  politicsNews   \n","1  Georgetown University has a 16.4% acceptance r...     left-news   \n","2  WASHINGTON (Reuters) - Former Central Intellig...  politicsNews   \n","3  Thus the big win for Donald Trump in blue stat...      politics   \n","4  Nothing pisses me off more than a Republican p...          News   \n","\n","                 date  label  \n","0  February 11, 2016       1  \n","1         Sep 1, 2016      0  \n","2       May 18, 2017       1  \n","3         Mar 8, 2016      0  \n","4      March 12, 2016      0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# creating target variable\n","df_true['label'] = 1\n","df_fake['label'] = 0\n","\n","# concatenating in one single dataframe\n","df = df_true.append(df_fake)\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:06:54.833621Z","iopub.status.busy":"2023-05-06T15:06:54.833076Z","iopub.status.idle":"2023-05-06T15:06:55.009566Z","shell.execute_reply":"2023-05-06T15:06:55.008532Z","shell.execute_reply.started":"2023-05-06T15:06:54.833589Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Black U.S. lawmakers group endorses Clinton Wh...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GEORGETOWN University Will Track Down, Recruit...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ex-CIA chief Brennan to testify before House I...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SHOCKING NUMBER OF MICHIGAN VOTERS Want MUSLIM...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Douchebag of the Day: Louie ‘Self Loathing Je...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  Black U.S. lawmakers group endorses Clinton Wh...      1\n","1  GEORGETOWN University Will Track Down, Recruit...      0\n","2  Ex-CIA chief Brennan to testify before House I...      1\n","3  SHOCKING NUMBER OF MICHIGAN VOTERS Want MUSLIM...      0\n","4   Douchebag of the Day: Louie ‘Self Loathing Je...      0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df['text'] = df['title'] + \" \" + df['text']\n","df = df[['text', 'label']]\n","df = df.reset_index(drop=True)\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:07:02.542020Z","iopub.status.busy":"2023-05-06T15:07:02.541690Z","iopub.status.idle":"2023-05-06T15:07:02.548040Z","shell.execute_reply":"2023-05-06T15:07:02.547069Z","shell.execute_reply.started":"2023-05-06T15:07:02.541992Z"},"trusted":true},"outputs":[],"source":["new_df=df.copy()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:07:06.759396Z","iopub.status.busy":"2023-05-06T15:07:06.759041Z","iopub.status.idle":"2023-05-06T15:07:06.765226Z","shell.execute_reply":"2023-05-06T15:07:06.764312Z","shell.execute_reply.started":"2023-05-06T15:07:06.759367Z"},"trusted":true},"outputs":[],"source":["def review_cleaning(text):\n","    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","    and remove words containing numbers.'''\n","    text = str(text).lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:07:09.863595Z","iopub.status.busy":"2023-05-06T15:07:09.863121Z","iopub.status.idle":"2023-05-06T15:07:29.200010Z","shell.execute_reply":"2023-05-06T15:07:29.199186Z","shell.execute_reply.started":"2023-05-06T15:07:09.863564Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>black us lawmakers group endorses clinton whit...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>georgetown university will track down recruit ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>excia chief brennan to testify before house in...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>shocking number of michigan voters want muslim...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>douchebag of the day louie ‘self loathing jew...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  black us lawmakers group endorses clinton whit...      1\n","1  georgetown university will track down recruit ...      0\n","2  excia chief brennan to testify before house in...      1\n","3  shocking number of michigan voters want muslim...      0\n","4   douchebag of the day louie ‘self loathing jew...      0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["new_df['text']=new_df['text'].apply(lambda x:review_cleaning(x))\n","new_df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:07:35.591588Z","iopub.status.busy":"2023-05-06T15:07:35.591248Z","iopub.status.idle":"2023-05-06T15:08:06.180009Z","shell.execute_reply":"2023-05-06T15:08:06.179065Z","shell.execute_reply.started":"2023-05-06T15:07:35.591561Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>black us lawmakers group endorses clinton whit...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>georgetown university track recruit admit stud...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>excia chief brennan testify house intelligence...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>shocking number michigan voters want muslim ba...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>douchebag day louie ‘self loathing jews’ gohme...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  black us lawmakers group endorses clinton whit...      1\n","1  georgetown university track recruit admit stud...      0\n","2  excia chief brennan testify house intelligence...      1\n","3  shocking number michigan voters want muslim ba...      0\n","4  douchebag day louie ‘self loathing jews’ gohme...      0"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["stop = stopwords.words('english')\n","new_df['text'] = new_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","new_df.head()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:08:39.681221Z","iopub.status.busy":"2023-05-06T15:08:39.680060Z","iopub.status.idle":"2023-05-06T15:08:41.975210Z","shell.execute_reply":"2023-05-06T15:08:41.974061Z","shell.execute_reply.started":"2023-05-06T15:08:39.681160Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","\n","is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    \n","print(device)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:08:44.770088Z","iopub.status.busy":"2023-05-06T15:08:44.769005Z","iopub.status.idle":"2023-05-06T15:08:44.779960Z","shell.execute_reply":"2023-05-06T15:08:44.779032Z","shell.execute_reply.started":"2023-05-06T15:08:44.770050Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["44898\n"]}],"source":["sentences = new_df['text'].values\n","print(len(sentences))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:09:09.374201Z","iopub.status.busy":"2023-05-06T15:09:09.373836Z","iopub.status.idle":"2023-05-06T15:09:10.586962Z","shell.execute_reply":"2023-05-06T15:09:10.586070Z","shell.execute_reply.started":"2023-05-06T15:09:09.374153Z"},"trusted":true},"outputs":[],"source":["from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AdamW"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:09:19.310743Z","iopub.status.busy":"2023-05-06T15:09:19.310382Z","iopub.status.idle":"2023-05-06T15:09:51.731522Z","shell.execute_reply":"2023-05-06T15:09:51.730436Z","shell.execute_reply.started":"2023-05-06T15:09:19.310714Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e1cb2680fa14554b6e7173d4b57807e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6403876bf0504f15bd3381cb9aad6c11","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2eb607b13d964f589f0e68f11db5e577","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9672747f806e4a3fae3ba4157d72a350","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model : \n","\n"," BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'bert-base-uncased' : 12 layer BERT model with uncased vocab\n","# num_labels : 2 labels for binary classification\n","tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert.cuda()\n","\n","print(\"\\n\\nBERT Model : \\n\\n\",model_bert)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:11:05.210235Z","iopub.status.busy":"2023-05-06T15:11:05.209209Z","iopub.status.idle":"2023-05-06T15:11:36.966965Z","shell.execute_reply":"2023-05-06T15:11:36.966071Z","shell.execute_reply.started":"2023-05-06T15:11:05.210166Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6852df52fc64e7f955ca306d4179506","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89df3523a1314957b9934cd2d287ba0b","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61a973478bff4a6eaa812c45e0914ad3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1f99baf3ee346ffb4a89f9e819a44a2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model : \n","\n"," RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'roberta-base' : 12 layer, 768 hidden, 12 heads, 125M params RoBERTa using BERT-base architecture\n","tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n","model_roberta.cuda()\n","\n","print(\"\\n\\nRoBERTa Model : \\n\\n\",model_roberta)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:11:43.350209Z","iopub.status.busy":"2023-05-06T15:11:43.349331Z","iopub.status.idle":"2023-05-06T15:19:00.136854Z","shell.execute_reply":"2023-05-06T15:19:00.134993Z","shell.execute_reply.started":"2023-05-06T15:11:43.350134Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Original: \n"," black us lawmakers group endorses clinton white house bid washington reuters democratic presidential candidate hillary clinton endorsed congressional black caucus thursday muchneeded boost campaign competes important black voting bloc south carolina’s primary month clinton husband former president bill clinton built strong ties africanamerican community past decades several lawmakers cited clinton’s long history working issues important black americans getting democrats elected advancing party’s agenda “mrs clinton demonstrated leadership skills labored various capacities adult life ready serve country occupying highest office country” said us representative gk butterfield group’s backing important clinton seeks recover tuesday’s stinging loss us senator bernie sanders new hampshire primary black voters make large bloc democratic primary south carolina increasingly significant democrats presidential contest moves iowa new hampshire demographically diverse states congressional black caucus political action committee planned send dozen members south carolina campaign clinton weekend advance state’s feb democratic primary sanders fresh victory new hampshire breakfast wednesday one america’s prominent civil rights activists rev al sharpton recognizing need broaden base mount longterm challenge clinton us representative james clyburn influential african american south carolina declined make personal endorsement caucus announcement washington appearing cnn thursday morning clyburn praised sanders’ voting record criticism us senator vermont comments suggested leaning toward clinton credited significant contributions issues important black americans particularly universal access health care children’s issues butterfield pointed alluding sanders promises universal health care free college tuition “we need president doesn’t simply campaign promise wonderful things things politically impossible achieve” said caucus announcement reporting doina chiacu editing bernadette baum alistair bell sap sponsor content independently created reuters’ editorial staff funded part sap otherwise role coverage\n","\n","Token IDs BERT: \n"," tensor([  101,  2304,  2149,  2375, 12088,  2177,  2203,  5668,  2229,  7207,\n","         2317,  2160,  7226,  2899, 26665,  3537,  4883,  4018, 18520,  7207,\n","        11763,  7740,  2304, 13965,  9432,  2172, 24045,  5732, 12992,  3049,\n","        14190,  2590,  2304,  6830, 15984,  2148,  3792,  1521,  1055,  3078,\n","         3204,  7207,  3129,  2280,  2343,  3021,  7207,  2328,  2844,  7208,\n","         3060, 14074, 14735,  2078,  2451,  2627,  5109,  2195,  2375, 12088,\n","         6563,  7207,  1521,  1055,  2146,  2381,  2551,  3314,  2590,  2304,\n","         4841,  2893,  8037,  2700, 10787,  2283,  1521,  1055, 11376,  1523,\n","         3680,  7207,  7645,  4105,  4813,  4450,  2098,  2536, 21157,  4639,\n","         2166,  3201,  3710,  2406, 13992,  3284,  2436,  2406,  1524,  2056,\n","         2149,  4387,  1043,  2243, 12136,  3790,  2177,  1521,  1055,  5150,\n","         2590,  7207, 11014,  8980,  9857,  1521,  1055, 22748,  3279,   102])\n","\n","Token IDs RoBERTa: \n"," tensor([    0, 14178,   201,  2648,   333,   253, 34225,  3741, 10528,  1104,\n","          790,  2311, 14784,  1054,   769, 13188,  7368,  1939,  1984,  9910,\n","         1766,  3741, 10528, 11585,  5744,   909, 17889,  3553, 46806,   203,\n","        14546,  2501,   637, 12766,   293,   505,   909,  3434,  7667,  2077,\n","          512, 27343,    17,    27,    29,  2270,   353,  3741, 10528,  1623,\n","          320,   394,  1087,  3741, 10528,  1490,   670,  3405,  9724, 45215,\n","         8015, 12657,   435,   375,  1724,   484,  2648,  4418,  3741, 10528,\n","           17,    27,    29,   251,   750,   447,   743,   505,   909, 38187,\n","         1253,   562, 38538,  2923,  2736, 11511,   537,    17,    27,    29,\n","         4026,    44,    48,   119,  4926,  3741, 10528,  7646,  1673,  2417,\n","         6348,  3995,  1337, 23549,  4194,   301,  1227,  1807,   247, 27183,\n","         1609,   558,   247,    17,    46,    26,   201,  4915,   821,     2])\n"]}],"source":["# BERT and RoBERTa are both Transformer models that have the same architecture. As such, they accept only a certain kind of inputs: vectors of integers, each value representing a token. Each string of text must first be converted to a list of indices to be fed to the model. The tokenizer takes care of that for us.\n","\n","# BERT and RoBERTa may have the same architecture, but they differ in tokenization. BERT uses a sub-word tokenization, whereas RoBERTa uses the same tokenization than GPT-2: byte-level byte-pair-encoding.\n","\n","\n","# **BERT Tokenizer**\n","\n","# Here, the BERT tokenizer splits the string into multiple substrings. If the substrings are in its vocabulary, they will stay as is: this is the case for `Cities`, `Taking` and `List`. However, if a resulting string is not in its vocabulary, it will be split again until every string is represented by its vocabulary. For example, `Transplanted` is split multiple times until every token is represented in the BERT vocabulary: it is split into three tokens.\n","# The BERT tokenizer is lacking when it comes to complex characters spread over multiple bytes, as can be seen with emojis. In the sequence used, an emoji of a Cityscape was added. As the BERT tokenizer cannot interpret this emoji on a byte-level, it replaces it by the unknown token [UNK].\n","\n","# **RoBERTa Tokenizer**\n","\n","# On the other hand, the RoBERTa tokenizer has a slightly different approach. Here too, the string is split into multiple substrings, which are themselves split into multiple substrings until every substring can be represented by the vocabulary. However, the RoBERTa tokenizer has a **byte-level approach**. This tokenizer can represent every sequence as a combination of bytes, which makes it shine in the case of complex characters spread over multiple bytes, as with the Cityscape emoji. Instead of using the unknown token, this tokenizer can correctly encode the Cityscape emoji as the combination of multiple bytes. This tokenizer therefore does not require an unknown token, as it can handle every byte separately.\n","\n","### Tokenization : encode_plus method from tokenizer_bert and tokenizer_roberta\n","\n","# 1. Split the sentence into tokens.\n","# 2. Add the special [CLS] and [SEP] tokens.\n","# 3. Map the tokens to their IDs.\n","# 4. Pad or truncate all sentences to the same length.\n","# 5. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n","final_labels = np.array(new_df['label'])\n","inputID_bert = []\n","attentionMask_bert = []\n","\n","inputID_roberta = []\n","attentionMask_roberta = []\n","\n","sentenceID = []\n","count = 0\n","\n","for text in sentences:\n","    \n","    enc_dict_bert = tokenizer_bert.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    enc_dict_roberta = tokenizer_roberta.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    \n","    # max_length : Pad and truncate all texts\n","    # return_attention_mask : construct attention masks\n","    # return_tensors : 'pt' : pytorch tensor\n","    \n","    inputID_bert.append(enc_dict_bert['input_ids'])\n","    inputID_roberta.append(enc_dict_roberta['input_ids']) # added encoded text as ID to the list\n","    \n","    attentionMask_bert.append(enc_dict_bert['attention_mask']) # added attention mask to the list\n","    attentionMask_roberta.append(enc_dict_roberta['attention_mask']) # that simply differs padding from non-padding\n","\n","    sentenceID.append(count)\n","    count = count + 1\n","    \n","# convert lists to tensor\n","\n","inputID_bert = torch.cat(inputID_bert,dim=0)\n","inputID_roberta = torch.cat(inputID_roberta,dim=0)\n","attentionMask_bert = torch.cat(attentionMask_bert,dim=0)\n","attentionMask_roberta = torch.cat(attentionMask_roberta,dim=0)\n","\n","labels = torch.tensor(final_labels)\n","sentenceID = torch.tensor(sentenceID)\n","\n","print('\\nOriginal: \\n', sentences[0])\n","print('\\nToken IDs BERT: \\n', inputID_bert[0])\n","print('\\nToken IDs RoBERTa: \\n', inputID_roberta[0])\n","\n","\n","\n","# In the above code, the lists inputID_bert, inputID_roberta, attentionMask_bert, attentionMask_roberta, labels, and sentenceID are converted into tensors because tensors are the primary data structure used in PyTorch for efficient computation on GPUs."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:25:52.857373Z","iopub.status.busy":"2023-05-06T15:25:52.856992Z","iopub.status.idle":"2023-05-06T15:25:52.863818Z","shell.execute_reply":"2023-05-06T15:25:52.862160Z","shell.execute_reply.started":"2023-05-06T15:25:52.857343Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:25:56.345974Z","iopub.status.busy":"2023-05-06T15:25:56.345168Z","iopub.status.idle":"2023-05-06T15:25:56.355414Z","shell.execute_reply":"2023-05-06T15:25:56.354192Z","shell.execute_reply.started":"2023-05-06T15:25:56.345942Z"},"trusted":true},"outputs":[],"source":["def sid_remove_from_tensordataset(datatensor): \n","    \n","    inputID = []\n","    attentionMask = []\n","    label = []\n","    \n","    for sid,iid,amask,l in datatensor:\n","        inputID.append(iid.tolist())\n","        attentionMask.append(amask.tolist())\n","        label.append(l.tolist())\n","    \n","    inputID = torch.tensor(inputID)\n","    attentionMask = torch.tensor(attentionMask)\n","    label = torch.tensor(label)\n","    \n","    return TensorDataset(inputID,attentionMask,label)\n","    \n","# Get DataSetLoaders\n","def get_loaders(dataset,batch_size,b):\n","\n","    \"\"\"\n","    return the train, validation and test set loaders\n","    \"\"\"  \n","    #dataset = torch.utils.data.TensorDataset(data_tr, labels_tr)\n","    train_size = int(0.8 * len(dataset))\n","    val_size = int(0.1 * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","    #print(\"\\nTrain DataSet Size :\",train_size)\n","    #print(\"\\nValidation DataSet Size :\",val_size)\n","    #print(\"\\nTest DataSet Size :\",test_size)\n","    train_dataset, validation_dataset,test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","    \n","    if(b==1): # sid remove only in BERT model : b=1\n","        train_dataset = sid_remove_from_tensordataset(train_dataset)\n","        validation_dataset = sid_remove_from_tensordataset(validation_dataset)\n","        test_dataset = sid_remove_from_tensordataset(test_dataset)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=RandomSampler(train_dataset))\n","    valid_loader = DataLoader(validation_dataset, batch_size=batch_size,sampler=SequentialSampler(validation_dataset))    \n","    test_loader = DataLoader(test_dataset, batch_size=batch_size,sampler=SequentialSampler(test_dataset))\n","\n","    return train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:26:00.577624Z","iopub.status.busy":"2023-05-06T15:26:00.576673Z","iopub.status.idle":"2023-05-06T15:26:00.588117Z","shell.execute_reply":"2023-05-06T15:26:00.587012Z","shell.execute_reply.started":"2023-05-06T15:26:00.577570Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(y_pred, y_test):\n","    y_pred_flat = np.argmax(y_pred, axis=1).flatten()\n","    y_test_flat = y_test.flatten()\n","    return np.sum(y_pred_flat == y_test_flat) / len(y_test_flat)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:26:03.186693Z","iopub.status.busy":"2023-05-06T15:26:03.186290Z","iopub.status.idle":"2023-05-06T15:26:03.199324Z","shell.execute_reply":"2023-05-06T15:26:03.198326Z","shell.execute_reply.started":"2023-05-06T15:26:03.186662Z"},"trusted":true},"outputs":[],"source":["def train_model(model, optimizer, train_loader):\n","    # Set the model to training mode\n","    model.train()\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    # Iterate over the batches in the train_loader\n","    for iid, amask, labels in train_loader:\n","        # Move input tensors to the appropriate device (e.g., GPU)\n","        iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","        \n","        # Reset the gradients\n","        model.zero_grad()\n","        \n","        # Forward pass through the model\n","        loss, outputs = model(iid, token_type_ids=None, attention_mask=amask, labels=labels, return_dict=False)\n","        \n","        # Compute the loss\n","        epoch_loss += loss.item()\n","        \n","        # Backpropagation and optimization (updating the model parameters)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Compute the accuracy\n","        epoch_acc += get_accuracy(outputs.detach().cpu().numpy(), labels.to('cpu').numpy())\n","\n","    # Calculate the average loss and accuracy for the epoch\n","    train_loss = epoch_loss / len(train_loader)\n","    train_acc = epoch_acc / len(train_loader)\n","    \n","    return train_loss, train_acc\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:26:06.180101Z","iopub.status.busy":"2023-05-06T15:26:06.179760Z","iopub.status.idle":"2023-05-06T15:26:06.190055Z","shell.execute_reply":"2023-05-06T15:26:06.189157Z","shell.execute_reply.started":"2023-05-06T15:26:06.180074Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, loader):\n","    # Initialize variables to keep track of loss and accuracy\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # Set the model to evaluation mode\n","    model.eval()\n","    \n","    # Initialize lists to store true labels and predicted labels\n","    y_true = []\n","    y_pred = []\n","    \n","    # Disable gradient computation and perform inference\n","    with torch.no_grad():\n","        # Iterate over the batches in the loader\n","        for iid, amask, labels in loader:\n","            # Move input tensors to the device (e.g., GPU)\n","            iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","\n","            # Compute the loss and outputs of the model\n","            loss, outputs = model(iid, token_type_ids=None, attention_mask=amask, labels=labels, return_dict=False)\n","            \n","            # Accumulate the loss\n","            epoch_loss += loss.item()\n","            \n","            # Compute the accuracy\n","            epoch_acc += get_accuracy(outputs.detach().cpu().numpy(), labels.to('cpu').numpy())\n","            \n","            # Append true labels and predicted labels to the respective lists\n","            y_true.append(labels.to('cpu').numpy())\n","            y_pred.append(outputs.detach().cpu().numpy())\n","    \n","    # Calculate the average loss and accuracy over the entire loader\n","    loss = epoch_loss / len(loader)\n","    acc = epoch_acc / len(loader)  \n","    \n","    # Return the loss, accuracy, predicted labels, and true labels\n","    return loss, acc, y_pred, y_true\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:26:10.618823Z","iopub.status.busy":"2023-05-06T15:26:10.618156Z","iopub.status.idle":"2023-05-06T15:26:10.627394Z","shell.execute_reply":"2023-05-06T15:26:10.625929Z","shell.execute_reply.started":"2023-05-06T15:26:10.618790Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","def run_model(model, train_loader, validate_loader, test_loader, epochs, batch_size, optimizer):\n","   \n","    for epoch in range(epochs):\n","        train_loss, train_acc = train_model(model, optimizer, train_loader)\n","        valid_loss, valid_acc, _, _ = evaluate_model(model, validate_loader)\n","\n","        print(f'Epoch: {epoch+1:02}')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ##Evaluate the test accuracy\n","\n","    test_loss, test_acc, y_pred, y_true = evaluate_model(model, test_loader)\n","    print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","    \n","    flat_pred = np.concatenate(y_pred,axis=0)\n","    flat_pred = np.argmax(flat_pred,axis=1).flatten()\n","    flat_true = np.concatenate(y_true, axis=0)\n","    print(\"\\nConfusion Matrix : \\n\",confusion_matrix(flat_true,flat_pred))\n","    print(\"\\nClassification Report : \\n\", classification_report(flat_true,flat_pred))\n","    \n","NUM_EPOCHS = 1\n","batch_size = 20"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:34:56.145294Z","iopub.status.busy":"2023-05-06T15:34:56.144919Z","iopub.status.idle":"2023-05-06T15:43:12.774188Z","shell.execute_reply":"2023-05-06T15:43:12.773237Z","shell.execute_reply.started":"2023-05-06T15:34:56.145264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.026 | Train Acc: 99.38%\n","\t Val. Loss: 0.012 |  Val. Acc: 99.73%\n","\n","Test Loss: 0.015 | Test Acc: 99.76%\n","\n","Confusion Matrix : \n"," [[2318    0]\n"," [  11 2162]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00      2318\n","         1.0       1.00      0.99      1.00      2173\n","\n","    accuracy                           1.00      4491\n","   macro avg       1.00      1.00      1.00      4491\n","weighted avg       1.00      1.00      1.00      4491\n","\n"]}],"source":["# RUN BERT Model\n","print(\"\\n\\nBERT Model :\\n\")\n","dataset_bert = TensorDataset(sentenceID, inputID_bert, attentionMask_bert, labels)\n","train_loader_bert, validation_loader_bert, test_loader_bert = get_loaders(dataset_bert, batch_size, 1)\n","optimizer_bert = AdamW(model_bert.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_bert, train_loader_bert, validation_loader_bert, test_loader_bert, NUM_EPOCHS, batch_size, optimizer_bert)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T15:26:16.168157Z","iopub.status.busy":"2023-05-06T15:26:16.167807Z","iopub.status.idle":"2023-05-06T15:34:35.718625Z","shell.execute_reply":"2023-05-06T15:34:35.717676Z","shell.execute_reply.started":"2023-05-06T15:26:16.168118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.038 | Train Acc: 99.21%\n","\t Val. Loss: 0.021 |  Val. Acc: 99.71%\n","\n","Test Loss: 0.012 | Test Acc: 99.84%\n","\n","Confusion Matrix : \n"," [[2357    2]\n"," [   5 2127]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00      2359\n","         1.0       1.00      1.00      1.00      2132\n","\n","    accuracy                           1.00      4491\n","   macro avg       1.00      1.00      1.00      4491\n","weighted avg       1.00      1.00      1.00      4491\n","\n"]}],"source":["# RUN RoBERTa Model\n","print(\"\\n\\nRoBERTa Model :\\n\")\n","dataset_roberta = TensorDataset(inputID_roberta, attentionMask_roberta, labels)\n","train_loader_roberta, validation_loader_roberta, test_loader_roberta = get_loaders(dataset_roberta, batch_size, 0)\n","optimizer_roberta = AdamW(model_roberta.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_roberta, train_loader_roberta, validation_loader_roberta, test_loader_roberta, NUM_EPOCHS, batch_size, optimizer_roberta)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":4}
