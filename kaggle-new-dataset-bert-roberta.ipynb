{"cells":[{"cell_type":"code","execution_count":56,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-06T13:12:10.534562Z","iopub.status.busy":"2023-05-06T13:12:10.534199Z","iopub.status.idle":"2023-05-06T13:12:10.557560Z","shell.execute_reply":"2023-05-06T13:12:10.556556Z","shell.execute_reply.started":"2023-05-06T13:12:10.534532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/fake-news-222/train.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:15.000811Z","iopub.status.busy":"2023-05-06T13:12:15.000463Z","iopub.status.idle":"2023-05-06T13:12:15.007678Z","shell.execute_reply":"2023-05-06T13:12:15.006709Z","shell.execute_reply.started":"2023-05-06T13:12:15.000783Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import nltk\n","import re\n","import string\n","import seaborn as sns\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud,STOPWORDS\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:19.265849Z","iopub.status.busy":"2023-05-06T13:12:19.265187Z","iopub.status.idle":"2023-05-06T13:12:20.240716Z","shell.execute_reply":"2023-05-06T13:12:20.239770Z","shell.execute_reply.started":"2023-05-06T13:12:19.265815Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/fake-news-222/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:26.187010Z","iopub.status.busy":"2023-05-06T13:12:26.186489Z","iopub.status.idle":"2023-05-06T13:12:26.262252Z","shell.execute_reply":"2023-05-06T13:12:26.261234Z","shell.execute_reply.started":"2023-05-06T13:12:26.186980Z"},"trusted":true},"outputs":[],"source":["df['text'] = df['title'] + ' ' + df['text']\n","df = df[['text', 'label']]\n","df = df[df['text'].notna()]\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:29.581203Z","iopub.status.busy":"2023-05-06T13:12:29.580828Z","iopub.status.idle":"2023-05-06T13:12:29.586918Z","shell.execute_reply":"2023-05-06T13:12:29.586022Z","shell.execute_reply.started":"2023-05-06T13:12:29.581171Z"},"trusted":true},"outputs":[],"source":["new_df=df.copy()"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:33.965009Z","iopub.status.busy":"2023-05-06T13:12:33.963893Z","iopub.status.idle":"2023-05-06T13:12:33.971592Z","shell.execute_reply":"2023-05-06T13:12:33.970685Z","shell.execute_reply.started":"2023-05-06T13:12:33.964964Z"},"trusted":true},"outputs":[],"source":["def review_cleaning(text):\n","    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","    and remove words containing numbers.'''\n","    text = str(text).lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:12:36.863771Z","iopub.status.busy":"2023-05-06T13:12:36.863343Z","iopub.status.idle":"2023-05-06T13:12:53.532708Z","shell.execute_reply":"2023-05-06T13:12:53.531842Z","shell.execute_reply.started":"2023-05-06T13:12:36.863737Z"},"trusted":true},"outputs":[],"source":["new_df['text']=new_df['text'].apply(lambda x:review_cleaning(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:13:31.914017Z","iopub.status.busy":"2023-05-06T13:13:31.913683Z","iopub.status.idle":"2023-05-06T13:13:58.881215Z","shell.execute_reply":"2023-05-06T13:13:58.880361Z","shell.execute_reply.started":"2023-05-06T13:13:31.913991Z"},"trusted":true},"outputs":[],"source":["stop = stopwords.words('english')\n","new_df['text'] = new_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:14:38.775315Z","iopub.status.busy":"2023-05-06T13:14:38.774948Z","iopub.status.idle":"2023-05-06T13:14:38.781770Z","shell.execute_reply":"2023-05-06T13:14:38.780880Z","shell.execute_reply.started":"2023-05-06T13:14:38.775288Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","\n","is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    \n","print(device)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:14:42.155519Z","iopub.status.busy":"2023-05-06T13:14:42.154965Z","iopub.status.idle":"2023-05-06T13:14:42.163512Z","shell.execute_reply":"2023-05-06T13:14:42.162085Z","shell.execute_reply.started":"2023-05-06T13:14:42.155490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["20203\n"]}],"source":["sentences = new_df['text'].values\n","print(len(sentences))"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:14:45.122808Z","iopub.status.busy":"2023-05-06T13:14:45.122462Z","iopub.status.idle":"2023-05-06T13:14:45.127114Z","shell.execute_reply":"2023-05-06T13:14:45.126109Z","shell.execute_reply.started":"2023-05-06T13:14:45.122780Z"},"trusted":true},"outputs":[],"source":["from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AdamW"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:14:48.137657Z","iopub.status.busy":"2023-05-06T13:14:48.137281Z","iopub.status.idle":"2023-05-06T13:14:50.206110Z","shell.execute_reply":"2023-05-06T13:14:50.205112Z","shell.execute_reply.started":"2023-05-06T13:14:48.137629Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model : \n","\n"," BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'bert-base-uncased' : 12 layer BERT model with uncased vocab\n","# num_labels : 2 labels for binary classification\n","tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert.cuda()\n","\n","print(\"\\n\\nBERT Model : \\n\\n\",model_bert)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:14:58.700922Z","iopub.status.busy":"2023-05-06T13:14:58.699950Z","iopub.status.idle":"2023-05-06T13:15:00.770895Z","shell.execute_reply":"2023-05-06T13:15:00.769908Z","shell.execute_reply.started":"2023-05-06T13:14:58.700877Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model : \n","\n"," RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'roberta-base' : 12 layer, 768 hidden, 12 heads, 125M params RoBERTa using BERT-base architecture\n","tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n","model_roberta.cuda()\n","\n","print(\"\\n\\nRoBERTa Model : \\n\\n\",model_roberta)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:15:05.363413Z","iopub.status.busy":"2023-05-06T13:15:05.363058Z","iopub.status.idle":"2023-05-06T13:21:11.842027Z","shell.execute_reply":"2023-05-06T13:21:11.841101Z","shell.execute_reply.started":"2023-05-06T13:15:05.363384Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Original: \n"," house dem aide didn’t even see comey’s letter jason chaffetz tweeted house dem aide didn’t even see comey’s letter jason chaffetz tweeted darrell lucus october subscribe jason chaffetz stump american fork utah image courtesy michael jolley available creative commonsby license apologies keith olbermann doubt worst person world week–fbi director james comey according house democratic aide looks like also know secondworst person well turns comey sent nowinfamous letter announcing fbi looking emails may related hillary clinton’s email server ranking democrats relevant committees didn’t hear comey found via tweet one republican committee chairmen know comey notified republican chairmen democratic ranking members house intelligence judiciary oversight committees agency reviewing emails recently discovered order see contained classified information long letter went oversight committee chairman jason chaffetz set political world ablaze tweet fbi dir informed fbi learned existence emails appear pertinent investigation case reopened — jason chaffetz jasoninthehouse october course know case comey actually saying reviewing emails light “an unrelated case”–which know anthony weiner’s sexting teenager apparently little things facts didn’t matter chaffetz utah republican already vowed initiate raft investigations hillary wins–at least two years’ worth possibly entire term’s worth apparently chaffetz thought fbi already work him–resulting tweet briefly roiled nation cooler heads realized dud according senior house democratic aide misreading letter may least chaffetz’ sins aide told shareblue boss democrats didn’t even know comey’s letter time–and found checked twitter “democratic ranking members relevant committees didn’t receive comey’s letter republican chairmen fact democratic ranking members didn’ receive chairman oversight government reform committee jason chaffetz tweeted made public” let’s see we’ve got right fbi director tells chaffetz gop committee chairmen major development potentially politically explosive investigation neither chaffetz colleagues courtesy let democratic counterparts know instead according aide made find twitter already talk daily kos comey provided advance notice letter chaffetz republicans giving time turn spin machine may make good theater nothing far even suggests case nothing far suggests comey anything grossly incompetent tonedeaf suggest however chaffetz acting way makes dan burton darrell issa look like models responsibility bipartisanship didn’t even decency notify ranking member elijah cummings something explosive doesn’t trample basic standards fairness don’t know granted it’s likely chaffetz answer sits ridiculously republican district anchored provo orem cook partisan voting index gave mitt romney punishing percent vote moreover republican house leadership given full support chaffetz’ planned fishing expedition doesn’t mean can’t turn hot lights textbook example house become republican control also second worst person world darrell lucus darrell graduate university north carolina considers journalist old school attempt turn member religious right college succeeded turning religious rights worst nightmarea charismatic christian unapologetic liberal desire stand scared silence increased survived abusive threeyear marriage may know daily kos christian dem nc follow twitter darrelllucus connect facebook click buy darrell mello yello connect\n","\n","Token IDs BERT: \n"," tensor([  101,  2160, 17183, 14895,  2134,  1521,  1056,  2130,  2156,  2272,\n","         2100,  1521,  1055,  3661,  4463, 15775, 16020,  5753,  1056, 28394,\n","         3064,  2160, 17183, 14895,  2134,  1521,  1056,  2130,  2156,  2272,\n","         2100,  1521,  1055,  3661,  4463, 15775, 16020,  5753,  1056, 28394,\n","         3064, 23158, 12776,  2271,  2255,  4942, 29234,  4463, 15775, 16020,\n","         5753, 22475,  2137,  9292,  6646,  3746, 14571,  2745,  8183, 25105,\n","         2800,  5541,  7674,  3762,  6105, 25380,  6766, 19330, 23991,  2078,\n","         4797,  5409,  2711,  2088,  2733,  1516,  8495,  2472,  2508,  2272,\n","         2100,  2429,  2160,  3537, 14895,  3504,  2066,  2036,  2113,  2117,\n","        12155, 12096,  2711,  2092,  4332,  2272,  2100,  2741,  2085,  2378,\n","         7011, 27711,  3661, 13856,  8495,  2559, 22028,  2089,  3141, 18520,\n","         7207,  1521,  1055, 10373,  8241,  5464,  8037,  7882,  9528,   102])\n","\n","Token IDs RoBERTa: \n"," tensor([    0,  3138,  4410,  9498,   399,    17,    27,    90,   190,   192,\n","          283,   219,    17,    27,    29,  1601,  1236, 12231,  1855,  3707,\n","        13126,  2858,   790,  4410,  9498,   399,    17,    27,    90,   190,\n","          192,   283,   219,    17,    27,    29,  1601,  1236, 12231,  1855,\n","         3707, 13126,  2858,   385, 29817, 23478,   687, 16874, 24761, 11222,\n","         1236, 12231,  1855,  3707, 13126, 33062, 38187,   260, 20935, 16080,\n","          895,  2274,  7676,   475, 25554,  1236,  1168,   607,   577,  3904,\n","        39758,  1409,  4385, 27620,  7321,  3432, 22796,  1943,  4621,  2980,\n","         2373,   621,   232,   186,  2383,   506,  5605,   736,  1236, 12336,\n","          283,   219,   309,   790,  7368,  9498,  1326,   101,    67,   216,\n","          200, 24390,   621,   157,  4072,   283,   219,  1051,   122,  9433,\n","        30681,  1601,  7106,   856,  5605,   546,  5575,   189,  1330,     2])\n"]}],"source":["### Tokenization : encode_plus method from tokenizer_bert and tokenizer_roberta\n","\n","# encode plus : tokenize sentence, prepand [CLS] to start, append [SEP] to end, \n","# map token to their ID, Pad or truncate the sentence to max_len, create attention masks for [PAD] tokens\n","final_labels = np.array(new_df['label'])\n","inputID_bert = []\n","attentionMask_bert = []\n","\n","inputID_roberta = []\n","attentionMask_roberta = []\n","\n","sentenceID = []\n","count = 0\n","\n","for text in sentences:\n","    \n","    enc_dict_bert = tokenizer_bert.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    enc_dict_roberta = tokenizer_roberta.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    \n","    # max_length : Pad and truncate all texts\n","    # return_attention_mask : construct attention masks\n","    # return_tensors : 'pt' : pytorch tensor\n","    \n","    inputID_bert.append(enc_dict_bert['input_ids'])\n","    inputID_roberta.append(enc_dict_roberta['input_ids']) # added encoded text as ID to the list\n","    \n","    attentionMask_bert.append(enc_dict_bert['attention_mask']) # added attention mask to the list\n","    attentionMask_roberta.append(enc_dict_roberta['attention_mask']) # that simply differs padding from non-padding\n","\n","    sentenceID.append(count)\n","    count = count + 1\n","    \n","# convert lists to tensor\n","\n","inputID_bert = torch.cat(inputID_bert,dim=0)\n","inputID_roberta = torch.cat(inputID_roberta,dim=0)\n","attentionMask_bert = torch.cat(attentionMask_bert,dim=0)\n","attentionMask_roberta = torch.cat(attentionMask_roberta,dim=0)\n","\n","labels = torch.tensor(final_labels)\n","sentenceID = torch.tensor(sentenceID)\n","\n","print('\\nOriginal: \\n', sentences[0])\n","print('\\nToken IDs BERT: \\n', inputID_bert[0])\n","print('\\nToken IDs RoBERTa: \\n', inputID_roberta[0])"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:42.733947Z","iopub.status.busy":"2023-05-06T13:21:42.733606Z","iopub.status.idle":"2023-05-06T13:21:42.738353Z","shell.execute_reply":"2023-05-06T13:21:42.737410Z","shell.execute_reply.started":"2023-05-06T13:21:42.733918Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:43.543644Z","iopub.status.busy":"2023-05-06T13:21:43.543292Z","iopub.status.idle":"2023-05-06T13:21:43.553584Z","shell.execute_reply":"2023-05-06T13:21:43.552718Z","shell.execute_reply.started":"2023-05-06T13:21:43.543619Z"},"trusted":true},"outputs":[],"source":["def sid_remove_from_tensordataset(datatensor): \n","    \n","    inputID = []\n","    attentionMask = []\n","    label = []\n","    \n","    for sid,iid,amask,l in datatensor:\n","        inputID.append(iid.tolist())\n","        attentionMask.append(amask.tolist())\n","        label.append(l.tolist())\n","    \n","    inputID = torch.tensor(inputID)\n","    attentionMask = torch.tensor(attentionMask)\n","    label = torch.tensor(label)\n","    \n","    return TensorDataset(inputID,attentionMask,label)\n","    \n","# Get DataSetLoaders\n","def get_loaders(dataset,batch_size,b):\n","\n","    \"\"\"\n","    return the train, validation and test set loaders\n","    \"\"\"  \n","    #dataset = torch.utils.data.TensorDataset(data_tr, labels_tr)\n","    train_size = int(0.8 * len(dataset))\n","    val_size = int(0.1 * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","    #print(\"\\nTrain DataSet Size :\",train_size)\n","    #print(\"\\nValidation DataSet Size :\",val_size)\n","    #print(\"\\nTest DataSet Size :\",test_size)\n","    train_dataset, validation_dataset,test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","    \n","    if(b==1): # sid remove only in BERT model : b=1\n","        train_dataset = sid_remove_from_tensordataset(train_dataset)\n","        validation_dataset = sid_remove_from_tensordataset(validation_dataset)\n","        test_dataset = sid_remove_from_tensordataset(test_dataset)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=RandomSampler(train_dataset))\n","    valid_loader = DataLoader(validation_dataset, batch_size=batch_size,sampler=SequentialSampler(validation_dataset))    \n","    test_loader = DataLoader(test_dataset, batch_size=batch_size,sampler=SequentialSampler(test_dataset))\n","\n","    return train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:48.657299Z","iopub.status.busy":"2023-05-06T13:21:48.656549Z","iopub.status.idle":"2023-05-06T13:21:48.662446Z","shell.execute_reply":"2023-05-06T13:21:48.661572Z","shell.execute_reply.started":"2023-05-06T13:21:48.657255Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(y_pred, y_test):\n","    y_pred_flat = np.argmax(y_pred, axis=1).flatten()\n","    y_test_flat = y_test.flatten()\n","    return np.sum(y_pred_flat == y_test_flat) / len(y_test_flat)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:51.364090Z","iopub.status.busy":"2023-05-06T13:21:51.363399Z","iopub.status.idle":"2023-05-06T13:21:51.371990Z","shell.execute_reply":"2023-05-06T13:21:51.371022Z","shell.execute_reply.started":"2023-05-06T13:21:51.364021Z"},"trusted":true},"outputs":[],"source":["def train_model(model, optimizer, train_loader):\n","  \n","    model.train()\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    for iid, amask, labels in train_loader:\n","        \n","        iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","        model.zero_grad()\n","        loss,outputs = model(iid,token_type_ids=None, attention_mask=amask,labels=labels, return_dict=False)  \n","        #torch.set_default_tensor_type(torch.FloatTensor)\n","        \n","        #x = torch.tensor(x, dtype=torch.float32)\n","        #labels=torch.tensor(labels, dtype= torch.float32)\n","        #loss = criterion(outputs, labels)\n","        #loss = criterion(outputs.squeeze(), labels.float32)\n","        epoch_loss += loss.item()\n","        loss.backward()\n","        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        \n","        epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n","\n","    train_loss =  epoch_loss / len(train_loader)\n","    train_acc = epoch_acc / len(train_loader)  \n","    return train_loss, train_acc"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:55.270092Z","iopub.status.busy":"2023-05-06T13:21:55.269724Z","iopub.status.idle":"2023-05-06T13:21:55.277949Z","shell.execute_reply":"2023-05-06T13:21:55.276738Z","shell.execute_reply.started":"2023-05-06T13:21:55.270056Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, loader):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    y_true=[]\n","    y_pred=[]\n","    \n","    with torch.no_grad():\n","    \n","        for iid, amask, labels in loader:\n","            iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","\n","            loss,outputs = model(iid,token_type_ids=None, attention_mask=amask, labels=labels, return_dict=False)\n","            #torch.set_default_tensor_type(torch.FloatTensor)\n","            labels=torch.tensor(labels, dtype= torch.float32)\n","            \n","            #e_loss = criterion(outputs.squeeze(), labels.float32)\n","            #e_loss = criterion(outputs,labels)\n","            epoch_loss += loss.item()\n","            \n","            epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n","           \n","            y_true.append(labels.to('cpu').numpy())\n","            y_pred.append(outputs.detach().cpu().numpy())\n","            \n","        \n","    loss =  epoch_loss / len(loader)\n","    acc = epoch_acc / len(loader)  \n","    return loss, acc, y_pred, y_true"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:21:58.524565Z","iopub.status.busy":"2023-05-06T13:21:58.524230Z","iopub.status.idle":"2023-05-06T13:21:58.532375Z","shell.execute_reply":"2023-05-06T13:21:58.531283Z","shell.execute_reply.started":"2023-05-06T13:21:58.524537Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","def run_model(model, train_loader, validate_loader, test_loader, epochs, batch_size, optimizer):\n","   \n","    for epoch in range(epochs):\n","        train_loss, train_acc = train_model(model, optimizer, train_loader)\n","        valid_loss, valid_acc, _, _ = evaluate_model(model, validate_loader)\n","\n","        print(f'Epoch: {epoch+1:02}')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ##Evaluate the test accuracy\n","\n","    test_loss, test_acc, y_pred, y_true = evaluate_model(model, test_loader)\n","    print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","    \n","    flat_pred = np.concatenate(y_pred,axis=0)\n","    flat_pred = np.argmax(flat_pred,axis=1).flatten()\n","    flat_true = np.concatenate(y_true, axis=0)\n","    print(\"\\nConfusion Matrix : \\n\",confusion_matrix(flat_true,flat_pred))\n","    print(\"\\nClassification Report : \\n\", classification_report(flat_true,flat_pred))\n","    \n","NUM_EPOCHS = 1\n","batch_size = 20"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:29:00.487216Z","iopub.status.busy":"2023-05-06T13:29:00.486854Z","iopub.status.idle":"2023-05-06T13:35:02.095599Z","shell.execute_reply":"2023-05-06T13:35:02.094400Z","shell.execute_reply.started":"2023-05-06T13:29:00.487184Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.102 | Train Acc: 96.17%\n","\t Val. Loss: 0.052 |  Val. Acc: 98.07%\n","\n","Test Loss: 0.044 | Test Acc: 98.43%\n","\n","Confusion Matrix : \n"," [[1031    8]\n"," [  24  958]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       0.98      0.99      0.98      1039\n","         1.0       0.99      0.98      0.98       982\n","\n","    accuracy                           0.98      2021\n","   macro avg       0.98      0.98      0.98      2021\n","weighted avg       0.98      0.98      0.98      2021\n","\n"]}],"source":["# RUN BERT Model\n","print(\"\\n\\nBERT Model :\\n\")\n","dataset_bert = TensorDataset(sentenceID, inputID_bert, attentionMask_bert, labels)\n","train_loader_bert, validation_loader_bert, test_loader_bert = get_loaders(dataset_bert, batch_size, 1)\n","optimizer_bert = AdamW(model_bert.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_bert, train_loader_bert, validation_loader_bert, test_loader_bert, NUM_EPOCHS, batch_size, optimizer_bert)"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T13:22:05.047993Z","iopub.status.busy":"2023-05-06T13:22:05.047647Z","iopub.status.idle":"2023-05-06T13:28:08.809159Z","shell.execute_reply":"2023-05-06T13:28:08.808294Z","shell.execute_reply.started":"2023-05-06T13:22:05.047964Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.147 | Train Acc: 94.38%\n","\t Val. Loss: 0.052 |  Val. Acc: 98.66%\n","\n","Test Loss: 0.059 | Test Acc: 98.28%\n","\n","Confusion Matrix : \n"," [[1059    7]\n"," [  28  927]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       0.97      0.99      0.98      1066\n","         1.0       0.99      0.97      0.98       955\n","\n","    accuracy                           0.98      2021\n","   macro avg       0.98      0.98      0.98      2021\n","weighted avg       0.98      0.98      0.98      2021\n","\n"]}],"source":["# RUN RoBERTa Model\n","print(\"\\n\\nRoBERTa Model :\\n\")\n","dataset_roberta = TensorDataset(inputID_roberta, attentionMask_roberta, labels)\n","train_loader_roberta, validation_loader_roberta, test_loader_roberta = get_loaders(dataset_roberta, batch_size, 0)\n","optimizer_roberta = AdamW(model_roberta.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_roberta, train_loader_roberta, validation_loader_roberta, test_loader_roberta, NUM_EPOCHS, batch_size, optimizer_roberta)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":4}
