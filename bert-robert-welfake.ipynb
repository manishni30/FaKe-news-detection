{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-05T09:58:46.055268Z","iopub.status.busy":"2023-05-05T09:58:46.054926Z","iopub.status.idle":"2023-05-05T09:58:46.065998Z","shell.execute_reply":"2023-05-05T09:58:46.065133Z","shell.execute_reply.started":"2023-05-05T09:58:46.055239Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/welfake-dataset/WELFake_Dataset.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:58:48.697762Z","iopub.status.busy":"2023-05-05T09:58:48.697436Z","iopub.status.idle":"2023-05-05T09:58:48.702000Z","shell.execute_reply":"2023-05-05T09:58:48.700872Z","shell.execute_reply.started":"2023-05-05T09:58:48.697734Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import nltk\n","import re\n","import string\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud,STOPWORDS\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:58:51.217768Z","iopub.status.busy":"2023-05-05T09:58:51.217430Z","iopub.status.idle":"2023-05-05T09:58:53.745366Z","shell.execute_reply":"2023-05-05T09:58:53.744352Z","shell.execute_reply.started":"2023-05-05T09:58:51.217740Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/welfake-dataset/WELFake_Dataset.csv')\n","df['text'] = df['title'] + \" \" + df['text']\n","df = df[['text', 'label']]\n","df = df[df['text'].notna()]\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_df=df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def review_cleaning(text):\n","    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","    and remove words containing numbers.'''\n","    text = str(text).lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_df['text']=new_df['text'].apply(lambda x:review_cleaning(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stop = stopwords.words('english')\n","new_df['text'] = new_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:58:59.724252Z","iopub.status.busy":"2023-05-05T09:58:59.723916Z","iopub.status.idle":"2023-05-05T09:59:04.904721Z","shell.execute_reply":"2023-05-05T09:59:04.903580Z","shell.execute_reply.started":"2023-05-05T09:58:59.724225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","\n","is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","    \n","print(device)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:59:07.731479Z","iopub.status.busy":"2023-05-05T09:59:07.730865Z","iopub.status.idle":"2023-05-05T09:59:07.740007Z","shell.execute_reply":"2023-05-05T09:59:07.738960Z","shell.execute_reply.started":"2023-05-05T09:59:07.731445Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["72095\n"]}],"source":["sentences = new_df['text'].values\n","print(len(sentences))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:59:11.617674Z","iopub.status.busy":"2023-05-05T09:59:11.617329Z","iopub.status.idle":"2023-05-05T09:59:26.975025Z","shell.execute_reply":"2023-05-05T09:59:26.974151Z","shell.execute_reply.started":"2023-05-05T09:59:11.617646Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AdamW"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:59:32.983815Z","iopub.status.busy":"2023-05-05T09:59:32.980261Z","iopub.status.idle":"2023-05-05T09:59:45.747341Z","shell.execute_reply":"2023-05-05T09:59:45.746483Z","shell.execute_reply.started":"2023-05-05T09:59:32.983774Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c42443d3c8549cfa062d73444332c85","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2121d86dc6a34c24946c815dd2f6019f","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8e8be6759d84703916e576572f1191d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"560325106834441ab043c37620dadcee","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model : \n","\n"," BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'bert-base-uncased' : 12 layer BERT model with uncased vocab\n","# num_labels : 2 labels for binary classification\n","tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert.cuda()\n","\n","print(\"\\n\\nBERT Model : \\n\\n\",model_bert)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T09:59:55.237875Z","iopub.status.busy":"2023-05-05T09:59:55.237456Z","iopub.status.idle":"2023-05-05T10:00:00.667694Z","shell.execute_reply":"2023-05-05T10:00:00.666698Z","shell.execute_reply.started":"2023-05-05T09:59:55.237836Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b9d2d5c611247cf93c72c6d3d53ca2c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"590c072d26d849f8a8a88d69cb42ac2b","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b23974fac84b480ebbeae60a80907a22","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d5f6f53c4f94ee590221a8773e4b730","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model : \n","\n"," RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=2,output_attentions=False,output_hidden_states=False)\n","# 'roberta-base' : 12 layer, 768 hidden, 12 heads, 125M params RoBERTa using BERT-base architecture\n","tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n","model_roberta.cuda()\n","\n","print(\"\\n\\nRoBERTa Model : \\n\\n\",model_roberta)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:00:09.333385Z","iopub.status.busy":"2023-05-05T10:00:09.332947Z","iopub.status.idle":"2023-05-05T10:21:44.831169Z","shell.execute_reply":"2023-05-05T10:21:44.830214Z","shell.execute_reply.started":"2023-05-05T10:00:09.333352Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Original: \n"," No comment is expected from Barack Obama Members of the #FYF911 or #FukYoFlag and #BlackLivesMatter movements called for the lynching and hanging of white people and cops. They encouraged others on a radio show Tuesday night to  turn the tide  and kill white people and cops to send a message about the killing of black people in America.One of the F***YoFlag organizers is called  Sunshine.  She has a radio blog show hosted from Texas called,  Sunshine s F***ing Opinion Radio Show. A snapshot of her #FYF911 @LOLatWhiteFear Twitter page at 9:53 p.m. shows that she was urging supporters to  Call now!! #fyf911 tonight we continue to dismantle the illusion of white Below is a SNAPSHOT Twitter Radio Call Invite   #FYF911The radio show aired at 10:00 p.m. eastern standard time.During the show, callers clearly call for  lynching  and  killing  of white people.A 2:39 minute clip from the radio show can be heard here. It was provided to Breitbart Texas by someone who would like to be referred to as  Hannibal.  He has already received death threats as a result of interrupting #FYF911 conference calls.An unidentified black man said  when those mother f**kers are by themselves, that s when when we should start f***ing them up. Like they do us, when a bunch of them ni**ers takin  one of us out, that s how we should roll up.  He said,  Cause we already roll up in gangs anyway. There should be six or seven black mother f**ckers, see that white person, and then lynch their ass. Let s turn the tables. They conspired that if  cops started losing people,  then  there will be a state of emergency. He speculated that one of two things would happen,  a big-ass [R s?????] war,  or  ni**ers, they are going to start backin  up. We are already getting killed out here so what the f**k we got to lose? Sunshine could be heard saying,  Yep, that s true. That s so f**king true. He said,  We need to turn the tables on them. Our kids are getting shot out here. Somebody needs to become a sacrifice on their side.He said,  Everybody ain t down for that s**t, or whatever, but like I say, everybody has a different position of war.  He continued,  Because they don t give a f**k anyway.  He said again,  We might as well utilized them for that s**t and turn the tables on these n**ers. He said, that way  we can start lookin  like we ain t havin  that many casualties, and there can be more causalities on their side instead of ours. They are out their killing black people, black lives don t matter, that s what those mother f**kers   so we got to make it matter to them. Find a mother f**ker that is alone. Snap his ass, and then f***in hang him from a damn tree. Take a picture of it and then send it to the mother f**kers. We  just need one example,  and  then people will start watchin .  This will turn the tables on s**t, he said. He said this will start  a trickle-down effect.  He said that when one white person is hung and then they are just  flat-hanging,  that will start the  trickle-down effect.  He continued,  Black people are good at starting trends. He said that was how  to get the upper-hand. Another black man spoke up saying they needed to kill  cops that are killing us. The first black male said,  That will be the best method right there. Breitbart Texas previously reported how Sunshine was upset when  racist white people  infiltrated and disrupted one of her conference calls. She subsequently released the phone number of one of the infiltrators. The veteran immediately started receiving threatening calls.One of the #F***YoFlag movement supporters allegedly told a veteran who infiltrated their publicly posted conference call,  We are going to rape and gut your pregnant wife, and your f***ing piece of sh*t unborn creature will be hung from a tree. Breitbart Texas previously encountered Sunshine at a Sandra Bland protest at the Waller County Jail in Texas, where she said all white people should be killed. She told journalists and photographers,  You see this nappy-ass hair on my head?   That means I am one of those more militant Negroes.  She said she was at the protest because  these redneck mother-f**kers murdered Sandra Bland because she had nappy hair like me. #FYF911 black radicals say they will be holding the  imperial powers  that are actually responsible for the terrorist attacks on September 11th accountable on that day, as reported by Breitbart Texas. There are several websites and Twitter handles for the movement. Palmetto Star  describes himself as one of the head organizers. He said in a YouTube video that supporters will be burning their symbols of  the illusion of their superiority,  their  false white supremacy,  like the American flag, the British flag, police uniforms, and Ku Klux Klan hoods.Sierra McGrone or  Nocturnus Libertus  posted,  you too can help a young Afrikan clean their a** with the rag of oppression.  She posted two photos, one that appears to be herself, and a photo of a black man, wiping their naked butts with the American flag.For entire story: Breitbart News\n","\n","Token IDs BERT: \n"," tensor([  101,  2053,  7615,  2003,  3517,  2013, 13857,  8112,  2372,  1997,\n","         1996,  1001,  1042,  2100,  2546,  2683, 14526,  2030,  1001, 11865,\n","         4801, 11253, 17802,  1998,  1001,  2304,  3669,  6961, 18900,  3334,\n","         5750,  2170,  2005,  1996, 11404,  2075,  1998,  5689,  1997,  2317,\n","         2111,  1998, 10558,  1012,  2027,  6628,  2500,  2006,  1037,  2557,\n","         2265,  9857,  2305,  2000,  2735,  1996, 10401,  1998,  3102,  2317,\n","         2111,  1998, 10558,  2000,  4604,  1037,  4471,  2055,  1996,  4288,\n","         1997,  2304,  2111,  1999,  2637,  1012,  2028,  1997,  1996,  1042,\n","         1008,  1008,  1008, 10930, 10258,  8490, 18829,  2003,  2170,  9609,\n","         1012,  2016,  2038,  1037,  2557,  9927,  2265,  4354,  2013,  3146,\n","         2170,  1010,  9609,  1055,  1042,  1008,  1008,  1008, 13749,  5448,\n","         2557,  2265,  1012,  1037, 20057, 12326,  1997,  2014,  1001,   102])\n","\n","Token IDs RoBERTa: \n"," tensor([    0,  3084,  1129,    16,   421,    31,  4282,  1284,  7845,     9,\n","            5,   849, 28409,   597, 37157,    50,   849,   597,  1350, 33543,\n","        46191,     8,   849, 11368,   574,  3699,   448,  7933,  7467,   373,\n","           13,     5, 23661,  7968,     8,  7209,     9,  1104,    82,     8,\n","        12620,     4,   252,  4446,   643,    15,    10,  3188,   311,   294,\n","          363,     7,  1437,  1004,     5, 13260,  1437,     8,  3549,  1104,\n","           82,     8, 12620,     7,  2142,    10,  1579,    59,     5,  2429,\n","            9,   909,    82,    11,   730,     4,  3762,     9,     5,   274,\n","        15057, 33543, 46191,  9921,    16,   373,  1437, 14995,     4,  1437,\n","          264,    34,    10,  3188,  5059,   311,  4457,    31,  1184,   373,\n","            6,  1437, 14995,   579,   274, 15057,   154, 20253,  4611,  2907,\n","            4,    83, 24512,     9,    69,   849, 28409,   597, 37157,     2])\n"]}],"source":["### Tokenization : encode_plus method from tokenizer_bert and tokenizer_roberta\n","\n","# encode plus : tokenize sentence, prepand [CLS] to start, append [SEP] to end, \n","# map token to their ID, Pad or truncate the sentence to max_len, create attention masks for [PAD] tokens\n","final_labels = np.array(new_df['label'])\n","inputID_bert = []\n","attentionMask_bert = []\n","\n","inputID_roberta = []\n","attentionMask_roberta = []\n","\n","sentenceID = []\n","count = 0\n","\n","for text in sentences:\n","    \n","    enc_dict_bert = tokenizer_bert.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    enc_dict_roberta = tokenizer_roberta.encode_plus(text,add_special_tokens=True,max_length=120,pad_to_max_length=True, return_attention_mask=True,return_tensors='pt')\n","    \n","    # max_length : Pad and truncate all texts\n","    # return_attention_mask : construct attention masks\n","    # return_tensors : 'pt' : pytorch tensor\n","    \n","    inputID_bert.append(enc_dict_bert['input_ids'])\n","    inputID_roberta.append(enc_dict_roberta['input_ids']) # added encoded text as ID to the list\n","    \n","    attentionMask_bert.append(enc_dict_bert['attention_mask']) # added attention mask to the list\n","    attentionMask_roberta.append(enc_dict_roberta['attention_mask']) # that simply differs padding from non-padding\n","\n","    sentenceID.append(count)\n","    count = count + 1\n","    \n","# convert lists to tensor\n","\n","inputID_bert = torch.cat(inputID_bert,dim=0)\n","inputID_roberta = torch.cat(inputID_roberta,dim=0)\n","attentionMask_bert = torch.cat(attentionMask_bert,dim=0)\n","attentionMask_roberta = torch.cat(attentionMask_roberta,dim=0)\n","\n","labels = torch.tensor(final_labels)\n","sentenceID = torch.tensor(sentenceID)\n","\n","print('\\nOriginal: \\n', sentences[0])\n","print('\\nToken IDs BERT: \\n', inputID_bert[0])\n","print('\\nToken IDs RoBERTa: \\n', inputID_roberta[0])"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:25:39.655624Z","iopub.status.busy":"2023-05-05T10:25:39.654554Z","iopub.status.idle":"2023-05-05T10:25:39.660434Z","shell.execute_reply":"2023-05-05T10:25:39.659282Z","shell.execute_reply.started":"2023-05-05T10:25:39.655583Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:25:43.135135Z","iopub.status.busy":"2023-05-05T10:25:43.134268Z","iopub.status.idle":"2023-05-05T10:25:43.143786Z","shell.execute_reply":"2023-05-05T10:25:43.142873Z","shell.execute_reply.started":"2023-05-05T10:25:43.135101Z"},"trusted":true},"outputs":[],"source":["def sid_remove_from_tensordataset(datatensor): \n","    \n","    inputID = []\n","    attentionMask = []\n","    label = []\n","    \n","    for sid,iid,amask,l in datatensor:\n","        inputID.append(iid.tolist())\n","        attentionMask.append(amask.tolist())\n","        label.append(l.tolist())\n","    \n","    inputID = torch.tensor(inputID)\n","    attentionMask = torch.tensor(attentionMask)\n","    label = torch.tensor(label)\n","    \n","    return TensorDataset(inputID,attentionMask,label)\n","    \n","# Get DataSetLoaders\n","def get_loaders(dataset,batch_size,b):\n","\n","    \"\"\"\n","    return the train, validation and test set loaders\n","    \"\"\"  \n","    #dataset = torch.utils.data.TensorDataset(data_tr, labels_tr)\n","    train_size = int(0.8 * len(dataset))\n","    val_size = int(0.1 * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","    #print(\"\\nTrain DataSet Size :\",train_size)\n","    #print(\"\\nValidation DataSet Size :\",val_size)\n","    #print(\"\\nTest DataSet Size :\",test_size)\n","    train_dataset, validation_dataset,test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","    \n","    if(b==1): # sid remove only in BERT model : b=1\n","        train_dataset = sid_remove_from_tensordataset(train_dataset)\n","        validation_dataset = sid_remove_from_tensordataset(validation_dataset)\n","        test_dataset = sid_remove_from_tensordataset(test_dataset)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=RandomSampler(train_dataset))\n","    valid_loader = DataLoader(validation_dataset, batch_size=batch_size,sampler=SequentialSampler(validation_dataset))    \n","    test_loader = DataLoader(test_dataset, batch_size=batch_size,sampler=SequentialSampler(test_dataset))\n","\n","    return train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:25:47.902210Z","iopub.status.busy":"2023-05-05T10:25:47.901850Z","iopub.status.idle":"2023-05-05T10:25:47.909322Z","shell.execute_reply":"2023-05-05T10:25:47.908484Z","shell.execute_reply.started":"2023-05-05T10:25:47.902181Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(y_pred, y_test):\n","    y_pred_flat = np.argmax(y_pred, axis=1).flatten()\n","    y_test_flat = y_test.flatten()\n","    return np.sum(y_pred_flat == y_test_flat) / len(y_test_flat)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:25:51.237863Z","iopub.status.busy":"2023-05-05T10:25:51.237108Z","iopub.status.idle":"2023-05-05T10:25:51.244605Z","shell.execute_reply":"2023-05-05T10:25:51.243665Z","shell.execute_reply.started":"2023-05-05T10:25:51.237828Z"},"trusted":true},"outputs":[],"source":["def train_model(model, optimizer, train_loader):\n","  \n","    model.train()\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    for iid, amask, labels in train_loader:\n","        \n","        iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","        model.zero_grad()\n","        loss,outputs = model(iid,token_type_ids=None, attention_mask=amask,labels=labels, return_dict=False)  \n","        #torch.set_default_tensor_type(torch.FloatTensor)\n","        \n","        #x = torch.tensor(x, dtype=torch.float32)\n","        #labels=torch.tensor(labels, dtype= torch.float32)\n","        #loss = criterion(outputs, labels)\n","        #loss = criterion(outputs.squeeze(), labels.float32)\n","        epoch_loss += loss.item()\n","        loss.backward()\n","        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        \n","        epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n","\n","    train_loss =  epoch_loss / len(train_loader)\n","    train_acc = epoch_acc / len(train_loader)  \n","    return train_loss, train_acc"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:25:55.759153Z","iopub.status.busy":"2023-05-05T10:25:55.758121Z","iopub.status.idle":"2023-05-05T10:25:55.766880Z","shell.execute_reply":"2023-05-05T10:25:55.766041Z","shell.execute_reply.started":"2023-05-05T10:25:55.759115Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, loader):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    y_true=[]\n","    y_pred=[]\n","    \n","    with torch.no_grad():\n","    \n","        for iid, amask, labels in loader:\n","            iid, amask, labels = iid.to(device), amask.to(device), labels.to(device)\n","\n","            loss,outputs = model(iid,token_type_ids=None, attention_mask=amask, labels=labels, return_dict=False)\n","            #torch.set_default_tensor_type(torch.FloatTensor)\n","            labels=torch.tensor(labels, dtype= torch.float32)\n","            \n","            #e_loss = criterion(outputs.squeeze(), labels.float32)\n","            #e_loss = criterion(outputs,labels)\n","            epoch_loss += loss.item()\n","            \n","            epoch_acc += get_accuracy(outputs.detach().cpu().numpy(),labels.to('cpu').numpy())\n","           \n","            y_true.append(labels.to('cpu').numpy())\n","            y_pred.append(outputs.detach().cpu().numpy())\n","            \n","        \n","    loss =  epoch_loss / len(loader)\n","    acc = epoch_acc / len(loader)  \n","    return loss, acc, y_pred, y_true"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:26:01.729374Z","iopub.status.busy":"2023-05-05T10:26:01.729018Z","iopub.status.idle":"2023-05-05T10:26:02.193225Z","shell.execute_reply":"2023-05-05T10:26:02.192290Z","shell.execute_reply.started":"2023-05-05T10:26:01.729346Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","def run_model(model, train_loader, validate_loader, test_loader, epochs, batch_size, optimizer):\n","   \n","    for epoch in range(epochs):\n","        train_loss, train_acc = train_model(model, optimizer, train_loader)\n","        valid_loss, valid_acc, _, _ = evaluate_model(model, validate_loader)\n","\n","        print(f'Epoch: {epoch+1:02}')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ##Evaluate the test accuracy\n","\n","    test_loss, test_acc, y_pred, y_true = evaluate_model(model, test_loader)\n","    print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","    \n","    flat_pred = np.concatenate(y_pred,axis=0)\n","    flat_pred = np.argmax(flat_pred,axis=1).flatten()\n","    flat_true = np.concatenate(y_true, axis=0)\n","    print(\"\\nConfusion Matrix : \\n\",confusion_matrix(flat_true,flat_pred))\n","    print(\"\\nClassification Report : \\n\", classification_report(flat_true,flat_pred))\n","    \n","NUM_EPOCHS = 1\n","batch_size = 20"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-04T19:21:12.268342Z","iopub.status.busy":"2023-05-04T19:21:12.267974Z","iopub.status.idle":"2023-05-04T19:34:32.365249Z","shell.execute_reply":"2023-05-04T19:34:32.364075Z","shell.execute_reply.started":"2023-05-04T19:21:12.268312Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","BERT Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.034 | Train Acc: 98.85%\n","\t Val. Loss: 0.030 |  Val. Acc: 98.95%\n","\n","Test Loss: 0.028 | Test Acc: 99.04%\n","\n","Confusion Matrix : \n"," [[3468   55]\n"," [  14 3673]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       1.00      0.98      0.99      3523\n","         1.0       0.99      1.00      0.99      3687\n","\n","    accuracy                           0.99      7210\n","   macro avg       0.99      0.99      0.99      7210\n","weighted avg       0.99      0.99      0.99      7210\n","\n"]}],"source":["# RUN BERT Model\n","print(\"\\n\\nBERT Model :\\n\")\n","dataset_bert = TensorDataset(sentenceID, inputID_bert, attentionMask_bert, labels)\n","train_loader_bert, validation_loader_bert, test_loader_bert = get_loaders(dataset_bert, batch_size, 1)\n","optimizer_bert = AdamW(model_bert.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_bert, train_loader_bert, validation_loader_bert, test_loader_bert, NUM_EPOCHS, batch_size, optimizer_bert)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-05-05T10:26:12.706380Z","iopub.status.busy":"2023-05-05T10:26:12.706005Z","iopub.status.idle":"2023-05-05T10:47:46.710539Z","shell.execute_reply":"2023-05-05T10:47:46.709362Z","shell.execute_reply.started":"2023-05-05T10:26:12.706349Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","RoBERTa Model :\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_31/1125999774.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels=torch.tensor(labels, dtype= torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 0.039 | Train Acc: 98.70%\n","\t Val. Loss: 0.016 |  Val. Acc: 99.61%\n","\n","Test Loss: 0.012 | Test Acc: 99.64%\n","\n","Confusion Matrix : \n"," [[3553    8]\n"," [  18 3631]]\n","\n","Classification Report : \n","               precision    recall  f1-score   support\n","\n","         0.0       0.99      1.00      1.00      3561\n","         1.0       1.00      1.00      1.00      3649\n","\n","    accuracy                           1.00      7210\n","   macro avg       1.00      1.00      1.00      7210\n","weighted avg       1.00      1.00      1.00      7210\n","\n"]}],"source":["# RUN RoBERTa Model\n","print(\"\\n\\nRoBERTa Model :\\n\")\n","dataset_roberta = TensorDataset(inputID_roberta, attentionMask_roberta, labels)\n","train_loader_roberta, validation_loader_roberta, test_loader_roberta = get_loaders(dataset_roberta, batch_size, 0)\n","optimizer_roberta = AdamW(model_roberta.parameters(),lr=5e-5,eps=1e-8)\n","\n","run_model(model_roberta, train_loader_roberta, validation_loader_roberta, test_loader_roberta, NUM_EPOCHS, batch_size, optimizer_roberta)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":4}
